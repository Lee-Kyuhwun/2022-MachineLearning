{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 반드시 처음부터 끝까지 스켈레톤 코드를 살펴보고 구현하기 시작하길 바란다\n\n## 1. 스켈레톤 코드를 [복사 및 편집] 하여 사용한다.\n## 2. 아래의 [Empty Module 3개]를 직접 구현한다.\n## (필수) 코드 작성 전에 Overview의 Description을 읽고, 본 프로젝트의 방향성을 이해하고 시작하세요.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:22:31.569503Z","iopub.execute_input":"2022-06-03T05:22:31.571197Z","iopub.status.idle":"2022-06-03T05:22:31.580476Z","shell.execute_reply.started":"2022-06-03T05:22:31.571147Z","shell.execute_reply":"2022-06-03T05:22:31.579704Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# 본 프로젝트를 위한 패키지 로드\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:22:31.600035Z","iopub.execute_input":"2022-06-03T05:22:31.601029Z","iopub.status.idle":"2022-06-03T05:22:31.605958Z","shell.execute_reply.started":"2022-06-03T05:22:31.600987Z","shell.execute_reply":"2022-06-03T05:22:31.604956Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# 자연어 처리를 위한 라이브러리 다운로드\n- nltk 라이브러리에서 punkt 데이터를 다운 받음, 이 데이터는 영화 리뷰와 같은 문서 데이터로 문자의 tokeninizer를 위해서 필요하다\n- nltk 라이브러리를 이용해서 불용어를 다운 받음","metadata":{}},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:22:31.633957Z","iopub.execute_input":"2022-06-03T05:22:31.634361Z","iopub.status.idle":"2022-06-03T05:22:31.640902Z","shell.execute_reply.started":"2022-06-03T05:22:31.634331Z","shell.execute_reply":"2022-06-03T05:22:31.640148Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# 데이터 로드","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/2022-ml-project2/train.csv\",encoding=\"latin-1\")\ndf_test = pd.read_csv(\"../input/2022-ml-project2/test.csv\",encoding=\"latin-1\")","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:22:31.675485Z","iopub.execute_input":"2022-06-03T05:22:31.675962Z","iopub.status.idle":"2022-06-03T05:22:31.696701Z","shell.execute_reply.started":"2022-06-03T05:22:31.675929Z","shell.execute_reply":"2022-06-03T05:22:31.695683Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:22:31.717169Z","iopub.execute_input":"2022-06-03T05:22:31.718315Z","iopub.status.idle":"2022-06-03T05:22:31.730568Z","shell.execute_reply.started":"2022-06-03T05:22:31.718073Z","shell.execute_reply":"2022-06-03T05:22:31.729079Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"X_train = df_train[\"Data\"]\ny_train = df_train[\"Class\"]\nX_test = df_test[\"Data\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:22:31.748851Z","iopub.execute_input":"2022-06-03T05:22:31.749161Z","iopub.status.idle":"2022-06-03T05:22:31.754757Z","shell.execute_reply.started":"2022-06-03T05:22:31.749128Z","shell.execute_reply":"2022-06-03T05:22:31.753677Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# [Empty Module #1] 텍스트 데이터 전처리  \n\n목표: 텍스트 데이터를 처리하기 위한 여러 과정을 거쳐, 머신을 위한 데이터를 만든다. \n\n```\n[input]\n--------------\n- text: 텍스트 문장 데이터 \n\n[output]\n--------------\n- text: 전처리를 완료한 문장 데이터 \n    \n```","metadata":{}},{"cell_type":"code","source":"# ------------------------------------------------\n# [Empty Module #1] 텍스트 데이터 전처리\n# ------------------------------------------------\nfrom nltk.corpus import stopwords \nfrom nltk.stem.porter import PorterStemmer\n\ndef data_processing(text):\n    # ------------------------------------------------------------\n    # 구현 가이드라인 \n    # ------------------------------------------------------------\n    # [1] re.sub 사용해 text 속 '[^A-Za-z]' 외의 문자만을 찾아내 제거한후, pre_words 변수에 저장\n    #      1) pattern은 '[^A-Za-z]', repl=' ' 로 각각 설정.\n    #      2) 이모지나 숫자,점과 같은 문자외의 것들을 제거했다. (이모지는 감정 분석과 관련해서 몇가지 의미를 나타내지만 이 테스크에서는 이러한 의미도 제거함.)\n    #\n    # [2] pre_words의 lower 내장 함수를 이용해 대문자들은 소문자로 변경\n    #      1)  대, 소문자가 구분되어 있으면 \"Go\"와 \"go\" 와 같이 동일한 단어를 머신은 다른 단어로 취급한다. 따라서 대문자를 모두 소문자로 변경.\n    #\n    # [3] word_tokenize 함수를 이용해 pre_word 를 토큰화하여 word를 리스트화한 후 tokenized_words변수에 저장\n    #\n    # [4] nltk 라이브러리로 다운 받은 stopwords의 \"words\" 내장 함수를 이용해 english 불용어를 찾아서 stops 변수에 저장  \n    #      1) 불용어: 텍스트 분류에서 불용어는 텍스트의 중요도을 결정하는데 영향을 미치지 않는 단어임. \n    #                    (ex: the, we, a , will), 따라서 불필요한 단어가 예측 모델에 악영향을 끼칠 수 있기 때문에 제거.\n    #\n    # [5] [3] 에서 찾은 문자열 중 단어가 [4] 에서 찾은 불용어 속에 없을 경우, tokenized_words_remove 리스트에 append \n    #\n    # [6] tokenized_words_remove의 단어를 PorterStemmer 속 stem 내장 함수를 이용해, 동일 의미를 갖는 단어를 동일한 단어로 변경하는 과정을 거친 후 다시 저장.\n    #    \n    # ------------------------------------------------------------\n    ##############\n    # [1]\n    pre_words = re.sub(r'[^A-Za-z]',' ',text)\n    #sub함수를 이용해서 문자를 제거한다\n    #여기서 [^A-Za-z]는 알파벳 A~Z, a~z이외는 전부 제거하라는 의미이다.\n    ##############\n    # [2]\n    pre_words = pre_words.lower()\n    #대문자를 소문자로 변경\n    ##############\n    # [3]\n    tokenized_words = word_tokenize(pre_words)\n    #스페이스와 구두점으로 구분해서 토큰화한다.\n    ##############\n    # [4]\n    stops = stopwords.words('english')\n    #words함수를 이용하여 불용어찾아서 stops에 저장 \n    ##############\n    tokenized_words_remove=[]\n    \n    for w in tokenized_words: \n        # [5]\n        ##############################################\n        if w not in stops:\n             tokenized_words_remove.append(w)\n                 # [5] [3] 에서 찾은 문자열 중 단어가 [4] 에서 찾은 불용어 속에 없을 경우, tokenized_words_remove 리스트에 추가한다.\n        ##############################################\n\n    stemmer = PorterStemmer()\n    for i in range(len(tokenized_words_remove)):\n        # [6]\n        ##############################################\n         tokenized_words_remove[i]=stemmer.stem(tokenized_words_remove[i])\n        #동일 의미를 갖는 단어를 동일한 단어로 변경하는 과정을 거친 후 다시 저장.\n        ##############################################\n    \n    return ( \" \".join( tokenized_words_remove ))","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:22:31.783340Z","iopub.execute_input":"2022-06-03T05:22:31.783803Z","iopub.status.idle":"2022-06-03T05:22:31.793780Z","shell.execute_reply.started":"2022-06-03T05:22:31.783758Z","shell.execute_reply":"2022-06-03T05:22:31.792972Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"X_train = [data_processing(i) for i in X_train]\nX_test = [data_processing(i) for i in X_test]","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:22:31.811550Z","iopub.execute_input":"2022-06-03T05:22:31.812070Z","iopub.status.idle":"2022-06-03T05:22:35.853813Z","shell.execute_reply.started":"2022-06-03T05:22:31.812020Z","shell.execute_reply":"2022-06-03T05:22:35.853068Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# [Empty Module #2] Bag of Word \n\n목표: 문장 데이터를 머신을 학습하기 위한 실수화된 Feature로 변경하기로한다. \n\n- train 과 test 데이터 전부 type을 ('U')로 변경하여 Countvectorizer를 사용한다. \n- [설명 링크](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer)","metadata":{}},{"cell_type":"code","source":"# ------------------------------------------------\n# [Empty Module #2] 텍스트 데이터 Bag of word  feature  화 \n# ------------------------------------------------\nfrom sklearn.feature_extraction.text import CountVectorizer\n# ------------------------------------------------------------\n# 구현 가이드라인 \n# ------------------------------------------------------------\n# [1] CountVectorizer를 정의\n#           1) max_features를 100으로 지정 \n#\n# [2] X_train 과 X_test를 numpy array로 변환 후 데이터 타입을 \"U\"로 변경해 저장\n#\n# [3] CountVectorizer를 이용해 X_train은 학습 및 변환(fit_transform)을 하고, X_test는 변환(transform)을 진행. \n# ------------------------------------------------------------\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n##############\n# [1]\nvectorizer =CountVectorizer(max_features=100)\n#CountVectorizer를 호출한다. 텍스트 문서 모음을 토큰 수의 행렬로 변환합니다.\n##############\n\n# [2]\n##############################################\nX_train = np.array(X_train).astype(\"U\")\nX_test = np.array(X_test).astype(\"U\")\n#x-train, x-test를 각각 array로 변환한후 데이터 타입을 U로 변환한다. U는 유니코드를 의미한다.\n##############################################\n\n##############\n# [3]\nX_train_features = vectorizer.fit_transform(X_train)\nX_test_features = vectorizer.transform(X_test)\n#CountVectorizer를 이용해 X_train은 학습 및 변환(fit_transform)을 하고, X_test는 변환(transform)을 진행한다.\n##############","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:22:35.856211Z","iopub.execute_input":"2022-06-03T05:22:35.857069Z","iopub.status.idle":"2022-06-03T05:22:35.979549Z","shell.execute_reply.started":"2022-06-03T05:22:35.856982Z","shell.execute_reply":"2022-06-03T05:22:35.978490Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# ham의 경우 0으로 지정하고, spam의 경우에는 1로 라벨을 변경해줌\ny_train[y_train==\"ham\"] = 0\ny_train[y_train==\"spam\"] = 1\ny_train = y_train.astype(\"uint8\")","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:22:35.981159Z","iopub.execute_input":"2022-06-03T05:22:35.981638Z","iopub.status.idle":"2022-06-03T05:22:35.996106Z","shell.execute_reply.started":"2022-06-03T05:22:35.981601Z","shell.execute_reply":"2022-06-03T05:22:35.995158Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## [Empty Module #3] SVM: classifier\n목표: SVC() 를 활용해 classification 을 진행\n\n- fit()으로 train 에 대한 머신러닝 학습\n- predict()으로 test 에 대한 정답을 추론 하여 반환","metadata":{}},{"cell_type":"code","source":"# ------------------------------------------------\n# [Empty Module #3] 텍스트 데이터 Bag of word  feature  화 \n# ------------------------------------------------\nfrom sklearn.svm import SVC\n# ------------------------------------------------------------\n# 구현 가이드라인 \n# ------------------------------------------------------------\n# [1]  SVC 선언 (베이스라인 에서 gamma=\"auto\" 사용 )\n#\n# [2] X_train_features과 y_train으로 SVC 학습진행 후, X_test_features로 predict 진행\n#\n# [3] y_pred에 predict한 결과값 저장\n# ------------------------------------------------------------\n###########\n# [1]\nsvc=SVC(gamma=\"auto\")\n#svc선언\n##############\n\n# [2]\n##############################################\nsvc.fit(X_train_features,y_train)\n#x_train_feature와 y_train를 훈련\n##############################################\n\n##############\n# [3]\ny_pred=svc.predict(X_test_features)\n#predict한 결과저장\n##############","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:22:35.998030Z","iopub.execute_input":"2022-06-03T05:22:35.998620Z","iopub.status.idle":"2022-06-03T05:22:36.273968Z","shell.execute_reply.started":"2022-06-03T05:22:35.998568Z","shell.execute_reply":"2022-06-03T05:22:36.273182Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Predict to CSV","metadata":{}},{"cell_type":"code","source":"df_pred={\"ID\": range(np.array(y_pred).shape[0]),\"Class\":y_pred}\ndf_pred=pd.DataFrame(df_pred)\ndf_pred.to_csv(\"predict.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:22:36.275178Z","iopub.execute_input":"2022-06-03T05:22:36.275717Z","iopub.status.idle":"2022-06-03T05:22:36.287056Z","shell.execute_reply.started":"2022-06-03T05:22:36.275676Z","shell.execute_reply":"2022-06-03T05:22:36.285840Z"},"trusted":true},"execution_count":36,"outputs":[]}]}