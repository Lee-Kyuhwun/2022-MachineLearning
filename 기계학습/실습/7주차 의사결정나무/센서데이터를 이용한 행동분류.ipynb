{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f414d5e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-22T17:59:22.767869Z",
     "iopub.status.busy": "2022-04-22T17:59:22.767322Z",
     "iopub.status.idle": "2022-04-22T17:59:22.814947Z",
     "shell.execute_reply": "2022-04-22T17:59:22.810784Z"
    },
    "papermill": {
     "duration": 0.08019,
     "end_time": "2022-04-22T17:59:22.818739",
     "exception": false,
     "start_time": "2022-04-22T17:59:22.738549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/2022-ml-w7p1/Feature_engineering.py\n",
      "/kaggle/input/2022-ml-w7p1/activity_labels.txt\n",
      "/kaggle/input/2022-ml-w7p1/features_info.txt\n",
      "/kaggle/input/2022-ml-w7p1/data_loader.py\n",
      "/kaggle/input/2022-ml-w7p1/features.txt\n",
      "/kaggle/input/2022-ml-w7p1/submit.csv\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp28_user14.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp45_user22.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp15_user08.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp05_user03.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp13_user07.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp43_user21.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp11_user06.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp26_user13.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp39_user19.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp05_user03.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/label_train.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp13_user07.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp54_user27.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp40_user20.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp61_user30.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp32_user16.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp38_user19.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp35_user17.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp18_user09.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp23_user11.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp59_user29.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp19_user10.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp46_user23.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp41_user20.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp17_user09.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp28_user14.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp29_user14.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp50_user25.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp11_user06.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp06_user03.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp26_user13.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp44_user22.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp60_user30.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp36_user18.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp60_user30.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp46_user23.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp45_user22.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp18_user09.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp08_user04.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp49_user24.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp06_user03.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp42_user21.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp37_user18.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp50_user25.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp40_user20.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp32_user16.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp53_user26.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp52_user26.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp27_user13.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp52_user26.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp01_user01.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp12_user06.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp44_user22.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp09_user05.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/label_test.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp01_user01.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp27_user13.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp59_user29.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp49_user24.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp51_user25.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp21_user10.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp20_user10.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp25_user12.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp12_user06.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp55_user27.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp23_user11.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp24_user12.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp22_user11.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp31_user15.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp19_user10.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp03_user02.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp04_user02.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp33_user16.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp07_user04.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp53_user26.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp22_user11.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp16_user08.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp14_user07.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp07_user04.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp57_user28.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp09_user05.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp48_user24.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp56_user28.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp35_user17.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp02_user01.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp20_user10.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp03_user02.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp61_user30.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp51_user25.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp37_user18.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp36_user18.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp34_user17.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp48_user24.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp10_user05.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp08_user04.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp56_user28.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp41_user20.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp31_user15.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp16_user08.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp57_user28.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp42_user21.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp54_user27.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp34_user17.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp55_user27.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp04_user02.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp30_user15.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp43_user21.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp29_user14.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp47_user23.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp58_user29.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp39_user19.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp21_user10.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp14_user07.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp17_user09.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp02_user01.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp33_user16.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp30_user15.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp24_user12.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp15_user08.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp38_user19.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp25_user12.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/gyro_exp10_user05.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp47_user23.txt\n",
      "/kaggle/input/2022-ml-w7p1/RawData/acc_exp58_user29.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67a509a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T17:59:22.875862Z",
     "iopub.status.busy": "2022-04-22T17:59:22.875564Z",
     "iopub.status.idle": "2022-04-22T17:59:22.880383Z",
     "shell.execute_reply": "2022-04-22T17:59:22.879602Z"
    },
    "papermill": {
     "duration": 0.035254,
     "end_time": "2022-04-22T17:59:22.882831",
     "exception": false,
     "start_time": "2022-04-22T17:59:22.847577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c40b0e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T17:59:22.943889Z",
     "iopub.status.busy": "2022-04-22T17:59:22.943309Z",
     "iopub.status.idle": "2022-04-22T17:59:22.982237Z",
     "shell.execute_reply": "2022-04-22T17:59:22.981423Z"
    },
    "papermill": {
     "duration": 0.072174,
     "end_time": "2022-04-22T17:59:22.984582",
     "exception": false,
     "start_time": "2022-04-22T17:59:22.912408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "# Importing Pandas Library \n",
    "from glob import glob\n",
    "# import display() for better visualitions of DataFrames and arrays\n",
    "from IPython.display import display\n",
    "# import pyplot for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('bmh') # for better plots\n",
    "# import data_loader for data loading\n",
    "from data_loader import import_raw_signals, import_labels_file,normalize5,normalize2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5de6542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T17:59:23.040958Z",
     "iopub.status.busy": "2022-04-22T17:59:23.040319Z",
     "iopub.status.idle": "2022-04-22T17:59:23.046924Z",
     "shell.execute_reply": "2022-04-22T17:59:23.046261Z"
    },
    "papermill": {
     "duration": 0.037978,
     "end_time": "2022-04-22T17:59:23.049291",
     "exception": false,
     "start_time": "2022-04-22T17:59:23.011313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Raw_data_paths = sorted(glob(\"/kaggle/input/2022-ml-w7p1/RawData/*\"))\n",
    "Raw_acc_paths=Raw_data_paths[0:61]\n",
    "Raw_gyro_paths=Raw_data_paths[61:122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7895afc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T17:59:23.105604Z",
     "iopub.status.busy": "2022-04-22T17:59:23.105030Z",
     "iopub.status.idle": "2022-04-22T17:59:35.324792Z",
     "shell.execute_reply": "2022-04-22T17:59:35.323378Z"
    },
    "papermill": {
     "duration": 12.250902,
     "end_time": "2022-04-22T17:59:35.327563",
     "exception": false,
     "start_time": "2022-04-22T17:59:23.076661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_dic={}\n",
    "raw_acc_columns=['acc_X','acc_Y','acc_Z']\n",
    "raw_gyro_columns=['gyro_X','gyro_Y','gyro_Z']\n",
    "for path_index in range(0,61):\n",
    "        key= Raw_data_paths[path_index][-16:-4]\n",
    "        raw_acc_data_frame=import_raw_signals(Raw_data_paths[path_index],raw_acc_columns)\n",
    "        raw_gyro_data_frame=import_raw_signals(Raw_data_paths[path_index+61],raw_gyro_columns)\n",
    "        raw_signals_data_frame=pd.concat([raw_acc_data_frame, raw_gyro_data_frame], axis=1)\n",
    "        raw_dic[key]=raw_signals_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6013b444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T17:59:35.381429Z",
     "iopub.status.busy": "2022-04-22T17:59:35.380004Z",
     "iopub.status.idle": "2022-04-22T17:59:35.398282Z",
     "shell.execute_reply": "2022-04-22T17:59:35.397306Z"
    },
    "papermill": {
     "duration": 0.047252,
     "end_time": "2022-04-22T17:59:35.400753",
     "exception": false,
     "start_time": "2022-04-22T17:59:35.353501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_raw_labels_columns=['experiment_number_ID','user_number_ID','activity_number_ID','Label_start_point','Label_end_point']\n",
    "test_raw_labels_columns=['experiment_number_ID','user_number_ID','Label_start_point','Label_end_point']\n",
    "\n",
    "test_labels_path=Raw_data_paths[122]\n",
    "train_labels_path=Raw_data_paths[123]\n",
    "\n",
    "train_Labels_Data_Frame=import_labels_file(train_labels_path,train_raw_labels_columns)\n",
    "test_Labels_Data_Frame=import_labels_file(test_labels_path,test_raw_labels_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4a0367",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T17:59:35.453367Z",
     "iopub.status.busy": "2022-04-22T17:59:35.453050Z",
     "iopub.status.idle": "2022-04-22T17:59:36.509944Z",
     "shell.execute_reply": "2022-04-22T17:59:36.508445Z"
    },
    "papermill": {
     "duration": 1.087408,
     "end_time": "2022-04-22T17:59:36.513597",
     "exception": false,
     "start_time": "2022-04-22T17:59:35.426189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.signal import medfilt\n",
    "\n",
    "def median(signal):\n",
    "    array=np.array(signal)   \n",
    "    med_filtered=sp.signal.medfilt(array, kernel_size=3)\n",
    "    return  med_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c32be83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T17:59:36.579059Z",
     "iopub.status.busy": "2022-04-22T17:59:36.578761Z",
     "iopub.status.idle": "2022-04-22T17:59:36.613470Z",
     "shell.execute_reply": "2022-04-22T17:59:36.612291Z"
    },
    "papermill": {
     "duration": 0.070234,
     "end_time": "2022-04-22T17:59:36.616341",
     "exception": false,
     "start_time": "2022-04-22T17:59:36.546107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft  \n",
    "from scipy.fftpack import fftfreq\n",
    "from scipy.fftpack import ifft\n",
    "import math \n",
    "\n",
    "sampling_freq = 50\n",
    "nyq=sampling_freq/float(2)\n",
    "freq1 = 0.3\n",
    "freq2 = 20\n",
    "\n",
    "# Function name: components_selection_one_signal\n",
    "\n",
    "# Inputs: t_signal:1D numpy array (time domain signal); \n",
    "\n",
    "# Outputs: (total_component,t_DC_component , t_body_component, t_noise) \n",
    "#           type(1D array,1D array, 1D array)\n",
    "\n",
    "# cases to discuss: if the t_signal is an acceleration signal then the t_DC_component is the gravity component [Grav_acc]\n",
    "#                   if the t_signal is a gyro signal then the t_DC_component is not useful\n",
    "# t_noise component is not useful\n",
    "# if the t_signal is an acceleration signal then the t_body_component is the body's acceleration component [Body_acc]\n",
    "# if the t_signal is a gyro signal then the t_body_component is the body's angular velocity component [Body_gyro]\n",
    "\n",
    "def components_selection_one_signal(t_signal,freq1,freq2):\n",
    "    t_signal=np.array(t_signal)\n",
    "    t_signal_length=len(t_signal)\n",
    "    f_signal=fft(t_signal)\n",
    "    freqs=np.array(sp.fftpack.fftfreq(t_signal_length, d=1/float(sampling_freq)))# frequency values between [-25hz:+25hz]\n",
    "    \n",
    "    # DC_component: f_signal values having freq between [-0.3 hz to 0 hz] and from [0 hz to 0.3hz] \n",
    "    #                                                             (-0.3 and 0.3 are included)\n",
    "    \n",
    "    # noise components: f_signal values having freq between [-25 hz to 20 hz[ and from ] 20 hz to 25 hz] \n",
    "    #                                                               (-25 and 25 hz inculded 20hz and -20hz not included)\n",
    "    \n",
    "    # selecting body_component: f_signal values having freq between [-20 hz to -0.3 hz] and from [0.3 hz to 20 hz] \n",
    "    #                                                               (-0.3 and 0.3 not included , -20hz and 20 hz included)\n",
    "    \n",
    "    \n",
    "    f_DC_signal=[] # DC_component in freq domain\n",
    "    f_body_signal=[] # body component in freq domain numpy.append(a, a[0])\n",
    "    f_noise_signal=[] # noise in freq domain\n",
    "    \n",
    "    for i in range(len(freqs)):# iterate over all available frequencies\n",
    "        \n",
    "        # selecting the frequency value\n",
    "        freq=freqs[i]\n",
    "        \n",
    "        # selecting the f_signal value associated to freq\n",
    "        value= f_signal[i]\n",
    "        \n",
    "        # Selecting DC_component values \n",
    "        if abs(freq)>0.3:# testing if freq is outside DC_component frequency ranges\n",
    "            f_DC_signal.append(float(0)) # add 0 to  the  list if it was the case (the value should not be added)                                       \n",
    "        else: # if freq is inside DC_component frequency ranges \n",
    "            f_DC_signal.append(value) # add f_signal value to f_DC_signal list\n",
    "    \n",
    "        # Selecting noise component values \n",
    "        if (abs(freq)<=20):# testing if freq is outside noise frequency ranges \n",
    "            f_noise_signal.append(float(0)) # # add 0 to  f_noise_signal list if it was the case \n",
    "        else:# if freq is inside noise frequency ranges \n",
    "            f_noise_signal.append(value) # add f_signal value to f_noise_signal\n",
    "\n",
    "        # Selecting body_component values \n",
    "        if (abs(freq)<=0.3 or abs(freq)>20):# testing if freq is outside Body_component frequency ranges\n",
    "            f_body_signal.append(float(0))# add 0 to  f_body_signal list\n",
    "        else:# if freq is inside Body_component frequency ranges\n",
    "            f_body_signal.append(value) # add f_signal value to f_body_signal list\n",
    "    \n",
    "    ################### Inverse the transformation of signals in freq domain ########################\n",
    "    # applying the inverse fft(ifft) to signals in freq domain and put them in float format\n",
    "    t_DC_component= ifft(np.array(f_DC_signal)).real\n",
    "    t_body_component= ifft(np.array(f_body_signal)).real\n",
    "    t_noise=ifft(np.array(f_noise_signal)).real\n",
    "    \n",
    "    total_component=t_signal-t_noise # extracting the total component(filtered from noise) \n",
    "                                     #  by substracting noise from t_signal (the original signal).\n",
    "    \n",
    "    # return outputs mentioned earlier\n",
    "    return (total_component,t_DC_component,t_body_component,t_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4adbf30d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T17:59:36.669684Z",
     "iopub.status.busy": "2022-04-22T17:59:36.669399Z",
     "iopub.status.idle": "2022-04-22T17:59:36.675956Z",
     "shell.execute_reply": "2022-04-22T17:59:36.675054Z"
    },
    "papermill": {
     "duration": 0.035161,
     "end_time": "2022-04-22T17:59:36.678376",
     "exception": false,
     "start_time": "2022-04-22T17:59:36.643215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def mag_3_signals(x,y,z): # Euclidian magnitude\n",
    "    return [math.sqrt((x[i]**2+y[i]**2+z[i]**2)) for i in range(len(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cce887b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T17:59:36.730511Z",
     "iopub.status.busy": "2022-04-22T17:59:36.730187Z",
     "iopub.status.idle": "2022-04-22T17:59:36.736657Z",
     "shell.execute_reply": "2022-04-22T17:59:36.735486Z"
    },
    "papermill": {
     "duration": 0.035263,
     "end_time": "2022-04-22T17:59:36.738909",
     "exception": false,
     "start_time": "2022-04-22T17:59:36.703646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dt=0.02 # dt=1/50=0.02s time duration between two rows\n",
    "# Input: 1D array with lenght=N (N:unknown)\n",
    "# Output: 1D array with lenght=N-1\n",
    "def jerk_one_signal(signal): \n",
    "        return np.array([(signal[i+1]-signal[i])/dt for i in range(len(signal)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "058a57fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T17:59:36.797621Z",
     "iopub.status.busy": "2022-04-22T17:59:36.797317Z",
     "iopub.status.idle": "2022-04-22T18:01:17.822453Z",
     "shell.execute_reply": "2022-04-22T18:01:17.821319Z"
    },
    "papermill": {
     "duration": 101.059146,
     "end_time": "2022-04-22T18:01:17.824913",
     "exception": false,
     "start_time": "2022-04-22T17:59:36.765767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [01:40<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "time_sig_dic={}\n",
    "raw_dic_keys=sorted(raw_dic.keys())\n",
    "\n",
    "import tqdm\n",
    "for key in tqdm.tqdm(raw_dic_keys):\n",
    "    raw_df=raw_dic[key]\n",
    "    time_sig_df=pd.DataFrame()\n",
    "    \n",
    "    for column in raw_df.columns:\n",
    "        t_signal=np.array(raw_df[column])\n",
    "        med_filtred=median(t_signal)\n",
    "        \n",
    "        if 'acc' in column:\n",
    "            _,grav_acc,body_acc,_=components_selection_one_signal(med_filtred,freq1,freq2)\n",
    "            body_acc_jerk=jerk_one_signal(body_acc)\n",
    "            time_sig_df['t_body_'+column]=body_acc[:-1]\n",
    "            time_sig_df['t_grav_'+column]= grav_acc[:-1]\n",
    "            time_sig_df['t_body_acc_jerk_'+column[-1]]=body_acc_jerk\n",
    "        elif 'gyro' in column:\n",
    "            _,_,body_gyro,_=components_selection_one_signal(med_filtred,freq1,freq2)\n",
    "            body_gyro_jerk=jerk_one_signal(body_gyro)\n",
    "            time_sig_df['t_body_gyro_'+column[-1]]=body_gyro[:-1]\n",
    "            time_sig_df['t_body_gyro_jerk_'+column[-1]]=body_gyro_jerk\n",
    "            \n",
    "    new_columns_ordered=['t_body_acc_X','t_body_acc_Y','t_body_acc_Z',\n",
    "                      't_grav_acc_X','t_grav_acc_Y','t_grav_acc_Z',\n",
    "                      't_body_acc_jerk_X','t_body_acc_jerk_Y','t_body_acc_jerk_Z',\n",
    "                      't_body_gyro_X','t_body_gyro_Y','t_body_gyro_Z',\n",
    "                      't_body_gyro_jerk_X','t_body_gyro_jerk_Y','t_body_gyro_jerk_Z']\n",
    "        \n",
    "    ordered_time_sig_df=pd.DataFrame()\n",
    "        \n",
    "    for col in new_columns_ordered:\n",
    "        ordered_time_sig_df[col]=time_sig_df[col]\n",
    "        \n",
    "    for i in range(0,15,3):\n",
    "        mag_col_name=new_columns_ordered[i][:-1]+'mag'\n",
    "        col0=np.array(ordered_time_sig_df[new_columns_ordered[i]]) # copy X_component\n",
    "        col1=ordered_time_sig_df[new_columns_ordered[i+1]] # copy Y_component\n",
    "        col2=ordered_time_sig_df[new_columns_ordered[i+2]] # copy Z_component\n",
    "        mag_signal=mag_3_signals(col0,col1,col2)\n",
    "        ordered_time_sig_df[mag_col_name]=mag_signal\n",
    "        \n",
    "    time_sig_dic[key]=ordered_time_sig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26a70aa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T18:01:17.934180Z",
     "iopub.status.busy": "2022-04-22T18:01:17.933886Z",
     "iopub.status.idle": "2022-04-22T18:01:17.943910Z",
     "shell.execute_reply": "2022-04-22T18:01:17.943280Z"
    },
    "papermill": {
     "duration": 0.064085,
     "end_time": "2022-04-22T18:01:17.945989",
     "exception": false,
     "start_time": "2022-04-22T18:01:17.881904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Windowing_type(time_sig_dic,Labels_Data_Frame):\n",
    "    columns=time_sig_dic['exp01_user01'].columns\n",
    "    window_ID=0\n",
    "    time_dictionary_window={}\n",
    "    BA_array=np.array(Labels_Data_Frame)\n",
    "    \n",
    "    for line in tqdm.tqdm(BA_array):\n",
    "        file_key= 'exp' + normalize2(int(line[0]))  +  '_user' + normalize2(int(line[1]))\n",
    "        \n",
    "        if line.shape[0] == 5 :\n",
    "          act_ID=line[2]\n",
    "          start_point=line[3]\n",
    "          end_point = line[4]\n",
    "        else :\n",
    "          act_ID='None'\n",
    "          start_point = line[2]\n",
    "          end_point = line[3]\n",
    "        \n",
    "        for cursor in range(start_point,end_point-127,64):\n",
    "            end_point=cursor+128\n",
    "            data=np.array(time_sig_dic[file_key].iloc[cursor:end_point])\n",
    "            window=pd.DataFrame(data=data,columns=columns)\n",
    "            key='t_W'+normalize5(window_ID)+'_'+file_key+'_act'+normalize2(act_ID)\n",
    "            time_dictionary_window[key]=window\n",
    "            window_ID=window_ID+1\n",
    "    \n",
    "    return time_dictionary_window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56417166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T18:01:18.049108Z",
     "iopub.status.busy": "2022-04-22T18:01:18.048569Z",
     "iopub.status.idle": "2022-04-22T18:01:23.951631Z",
     "shell.execute_reply": "2022-04-22T18:01:23.950902Z"
    },
    "papermill": {
     "duration": 5.958255,
     "end_time": "2022-04-22T18:01:23.953854",
     "exception": false,
     "start_time": "2022-04-22T18:01:17.995599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 598/598 [00:04<00:00, 147.99it/s]\n",
      "100%|██████████| 258/258 [00:01<00:00, 139.39it/s]\n"
     ]
    }
   ],
   "source": [
    "train_time_dictionary_window  = Windowing_type(time_sig_dic,train_Labels_Data_Frame)\n",
    "test_time_dictionary_window  = Windowing_type(time_sig_dic,test_Labels_Data_Frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21e65129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T18:01:24.106035Z",
     "iopub.status.busy": "2022-04-22T18:01:24.104785Z",
     "iopub.status.idle": "2022-04-22T18:01:24.110608Z",
     "shell.execute_reply": "2022-04-22T18:01:24.109913Z"
    },
    "papermill": {
     "duration": 0.08398,
     "end_time": "2022-04-22T18:01:24.112831",
     "exception": false,
     "start_time": "2022-04-22T18:01:24.028851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_window = train_time_dictionary_window[sorted(train_time_dictionary_window.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2424faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T18:01:24.257928Z",
     "iopub.status.busy": "2022-04-22T18:01:24.257623Z",
     "iopub.status.idle": "2022-04-22T18:01:24.265788Z",
     "shell.execute_reply": "2022-04-22T18:01:24.264285Z"
    },
    "papermill": {
     "duration": 0.084133,
     "end_time": "2022-04-22T18:01:24.268192",
     "exception": false,
     "start_time": "2022-04-22T18:01:24.184059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import fftpack\n",
    "from numpy.fft import *\n",
    "\n",
    "def fast_fourier_transform_one_signal(t_signal):\n",
    "    complex_f_signal= fftpack.fft(t_signal)\n",
    "    amplitude_f_signal=np.abs(complex_f_signal)\n",
    "    \n",
    "    return amplitude_f_signal\n",
    "\n",
    "def fast_fourier_transform(t_window):\n",
    "    f_window=pd.DataFrame()\n",
    "    for column in t_window.columns:\n",
    "        if 'grav' not in column:\n",
    "            t_signal=np.array(t_window[column])\n",
    "            f_signal= np.apply_along_axis(fast_fourier_transform_one_signal,0,t_signal)\n",
    "            f_window[\"f_\"+column[2:]]=f_signal\n",
    "    return f_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7772f27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T18:01:24.414934Z",
     "iopub.status.busy": "2022-04-22T18:01:24.414328Z",
     "iopub.status.idle": "2022-04-22T18:03:05.560306Z",
     "shell.execute_reply": "2022-04-22T18:03:05.559299Z"
    },
    "papermill": {
     "duration": 101.221635,
     "end_time": "2022-04-22T18:03:05.563620",
     "exception": false,
     "start_time": "2022-04-22T18:01:24.341985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7283/7283 [01:10<00:00, 103.34it/s]\n",
      "100%|██████████| 3116/3116 [00:30<00:00, 101.64it/s]\n"
     ]
    }
   ],
   "source": [
    "train_frequent_dictionary_window = {'f'+key[1:] : train_t_df.pipe(fast_fourier_transform) for key, train_t_df in tqdm.tqdm(train_time_dictionary_window.items())}\n",
    "test_frequent_dictionary_window = {'f'+key[1:] : test_t_df.pipe(fast_fourier_transform) for key, test_t_df in tqdm.tqdm(test_time_dictionary_window.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9ba94d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T18:03:06.409526Z",
     "iopub.status.busy": "2022-04-22T18:03:06.409262Z",
     "iopub.status.idle": "2022-04-22T18:03:06.414584Z",
     "shell.execute_reply": "2022-04-22T18:03:06.413630Z"
    },
    "papermill": {
     "duration": 0.421832,
     "end_time": "2022-04-22T18:03:06.416456",
     "exception": false,
     "start_time": "2022-04-22T18:03:05.994624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_window = train_frequent_dictionary_window[sorted(train_frequent_dictionary_window.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65473118",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T18:03:07.253976Z",
     "iopub.status.busy": "2022-04-22T18:03:07.253710Z",
     "iopub.status.idle": "2022-04-22T18:03:07.330590Z",
     "shell.execute_reply": "2022-04-22T18:03:07.329688Z"
    },
    "papermill": {
     "duration": 0.506191,
     "end_time": "2022-04-22T18:03:07.333102",
     "exception": false,
     "start_time": "2022-04-22T18:03:06.826911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# [Empty Module #1] Feature Engineering\n",
    "# -------------------------------------\n",
    "\n",
    "# -------------------------------------\n",
    "# Feature Engineering\n",
    "# -------------------------------------\n",
    "# 목적: 제공된 36개의 시퀀스 도메인 데이터를 기반으로 유의미한 피처를 추출한다.\n",
    "# 입력인자: 시간(time) 도메인 Feature 20개 , 주파수(frequency) 도메인 Feature 16개\n",
    "# 출력인자: 분류모델 학습을 위한 Feature\n",
    "# -------------------------------------\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 구현 가이드라인 - 논문에서 제안하는 Feature Engineering 방법\n",
    "# ------------------------------------------------------------\n",
    "#\n",
    "# mean(): Mean value\n",
    "# std(): Standard deviation\n",
    "# mad(): Median absolute deviation \n",
    "# max(): Largest value in array\n",
    "# min(): Smallest value in array\n",
    "# sma(): Signal magnitude area\n",
    "# energy(): Energy measure. Sum of the squares divided by the number of values. \n",
    "# iqr(): Interquartile range \n",
    "# entropy(): Signal entropy\n",
    "# arCoeff(): Autorregresion coefficients with Burg order equal to 4\n",
    "# correlation(): correlation coefficient between two signals\n",
    "# maxInds(): index of the frequency component with largest magnitude\n",
    "# meanFreq(): Weighted average of the frequency components to obtain a mean frequency\n",
    "# skewness(): skewness of the frequency domain signal \n",
    "# kurtosis(): kurtosis of the frequency domain signal \n",
    "# bandsEnergy(): Energy of a frequency interval within the 64 bins of the FFT of each window.\n",
    "# angle(): Angle between to vectors.\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "# Time domain Feature Extract function\n",
    "\n",
    "from Feature_engineering import mean_axial,std_axial,mad_axial,max_axial,min_axial, t_sma_axial, t_energy_axial,IQR_axial,entropy_axial, t_arburg_axial, t_corr_axial\n",
    "from Feature_engineering import mean_mag,std_mag,mad_mag,max_mag,min_mag,t_sma_mag,t_energy_mag,IQR_mag,entropy_mag,t_arburg_mag\n",
    "\n",
    "# Frequency domain Feature Extract function\n",
    "from Feature_engineering import f_sma_axial,f_energy_axial,f_max_Inds_axial,f_mean_Freq_axial,f_skewness_and_kurtosis_axial,f_all_bands_energy_axial\n",
    "from Feature_engineering import f_sma_mag,f_energy_mag,f_max_Inds_mag,f_mean_Freq_mag,f_skewness_mag,f_kurtosis_mag\n",
    "\n",
    "# Additional Feature Extract function\n",
    "from Feature_engineering import angle_features\n",
    "\n",
    "def feature_extractor(time_dictionary,freq_dictionary, condition='train') :\n",
    "    \n",
    "    \n",
    "    if condition is 'train' :\n",
    "        total_data = []\n",
    "        total_label = []\n",
    "    elif condition is 'test' :\n",
    "        total_data = []\n",
    "        \n",
    "    for i in tqdm.tqdm(range(len(time_dictionary))) :\n",
    "        \n",
    "        time_key = sorted(time_dictionary.keys())[i]\n",
    "        freq_key = sorted(freq_dictionary.keys())[i]\n",
    "        \n",
    "        time_window = time_dictionary[time_key]\n",
    "        freq_window = freq_dictionary[freq_key]\n",
    "        \n",
    "        if condition is 'train' :\n",
    "          window_user_id= int(time_key[-8:-6]) # extract the user id from window's key\n",
    "          window_activity_id=int(time_key[-2:]) # extract the activity id from the windows key\n",
    "        elif condition is 'test' :\n",
    "          window_user_id= int(time_key[-10:-8]) # extract the user id from window's key\n",
    "          window_activity_id= 0\n",
    "        else :\n",
    "            print(\"Error\")\n",
    "            sys.exit()\n",
    "            break;\n",
    "            \n",
    "        ##################################################################################\n",
    "        \n",
    "        \n",
    "        # Time domain - Feature extractor - Part 1. axial(X,Y,Z) Features \n",
    "        \n",
    "        #[0,1,2] : 't_body_acc_X', 't_body_acc_Y', 't_body_acc_Z'\n",
    "        #[3,4,5] : 't_grav_acc_X','t_grav_acc_Y', 't_grav_acc_Z'\n",
    "        #[6,7,8] : 't_body_acc_jerk_X','t_body_acc_jerk_Y', 't_body_acc_jerk_Z'\n",
    "        #[9,10,11] : 't_body_gyro_X','t_body_gyro_Y', 't_body_gyro_Z'\n",
    "        #[12,13,14] : 't_body_gyro_jerk_X', 't_body_gyro_jerk_Y', 't_body_gyro_jerk_Z'\n",
    "        \n",
    "        axial_columns = time_window.columns[0:15]\n",
    "        axial_df = time_window[axial_columns] # X,Y,Z\n",
    "        \n",
    "        time_axial_features = []\n",
    "        \n",
    "        for col in range(0,15,3) : \n",
    "            # ------------------------------------------------------------\n",
    "            # 구현 가이드라인 \n",
    "            # ------------------------------------------------------------\n",
    "            # 아래 time_3axial_vector 나타난 Feature를 계산하여야 한다.\n",
    "            # 각각을 계산하기위한 함수는 'Feature_engineering.py'에 내제되어 있다.\n",
    "            # ------------------------------------------------------------\n",
    "            curr_col = axial_columns[col:col+3]\n",
    "            curr_df = axial_df[curr_col]\n",
    "            \n",
    "            mean_vector = mean_axial(curr_df)\n",
    "            std_vector = std_axial(curr_df)\n",
    "            mad_vector = mad_axial(curr_df)\n",
    "            max_vector = max_axial(curr_df)\n",
    "            min_vector = min_axial(curr_df)\n",
    "            sma_value = t_sma_axial(curr_df)\n",
    "            energy_vector = t_energy_axial(curr_df)\n",
    "            IQR_vector = IQR_axial(curr_df)\n",
    "            entropy_vector = entropy_axial(curr_df)\n",
    "            AR_vector = t_arburg_axial(curr_df)\n",
    "            corr_vector = t_corr_axial(curr_df)\n",
    "            # 40 value per each 3-axial signals\n",
    "            time_3axial_vector = mean_vector + std_vector + mad_vector + \\\n",
    "                                 max_vector + min_vector + [sma_value] + \\\n",
    "                                 energy_vector + IQR_vector + entropy_vector + \\\n",
    "                                 AR_vector + corr_vector\n",
    "            \n",
    "            # append these features to the global list of features\n",
    "            time_axial_features= time_axial_features+time_3axial_vector\n",
    "        \n",
    "        ##################################################################################\n",
    "        \n",
    "        # Time domain - Feature extractor - Part 2. Magnitude Features \n",
    "        \n",
    "        #[15]'t_body_acc_mag'\n",
    "        #[16]'t_grav_acc_mag'\n",
    "        #[17]'t_body_acc_jerk_mag'\n",
    "        #[18]'t_body_gyro_mag'\n",
    "        #[19]'t_body_gyro_jerk_mag'\n",
    "        \n",
    "        mag_columns = time_window.columns[15:]\n",
    "        mag_columns = time_window[mag_columns]\n",
    "        \n",
    "        time_mag_features = []\n",
    "        \n",
    "        for ci, col in enumerate(mag_columns) :\n",
    "            \n",
    "            # ------------------------------------------------------------\n",
    "            # 구현 가이드라인 \n",
    "            # ------------------------------------------------------------\n",
    "            # 아래 col_mag_values 나타난 Feature를 계산하여야 한다.\n",
    "            # 각각을 계산하기위한 함수는 'Feature_engineering.py'에 내제되어 있다.\n",
    "            # ------------------------------------------------------------\n",
    "   \n",
    "            mean_value = mean_mag(mag_columns[col])\n",
    "            std_value = std_mag(mag_columns[col])\n",
    "            mad_value = mad_mag(mag_columns[col])\n",
    "            max_value = max_mag(mag_columns[col])\n",
    "            min_value = min_mag(mag_columns[col])\n",
    "            sma_value = f_sma_mag(mag_columns[col])\n",
    "            energy_value = t_energy_mag(mag_columns[col])\n",
    "            IQR_value = IQR_mag(mag_columns[col])\n",
    "            entropy_value = entropy_mag(mag_columns[col])\n",
    "            \n",
    "            curr_col = axial_columns[int(ci*3):int(ci*3)+3]\n",
    "            curr_df = axial_df[curr_col]\n",
    "            AR_vector = t_arburg_axial(curr_df)\n",
    "            \n",
    "            # 13 value per each t_mag_column\n",
    "            col_mag_values = [mean_value, std_value, mad_value, max_value, min_value, sma_value, \n",
    "                              energy_value,IQR_value, entropy_value]+ AR_vector\n",
    "\n",
    "            # col_mag_values will be added to the global list\n",
    "            time_mag_features= time_mag_features+ col_mag_values\n",
    "\n",
    "        \n",
    "        ##################################################################################\n",
    "        \n",
    "        # Frequency domain - Feature extractor - Part 1. axial(X,Y,Z) Features \n",
    "        \n",
    "        #[0,1,2] : 'f_body_acc_X', 'f_body_acc_Y', 'f_body_acc_Z'\n",
    "        #[3,4,5] : 'f_body_acc_jerk_X','f_body_acc_jerk_Y', 'f_body_acc_jerk_Z'\n",
    "        #[6,7,8] : 'f_body_gyro_X','f_body_gyro_Y', 'f_body_gyro_Z'\n",
    "        #[9,10,11] : 'f_body_gyro_jerk_X','f_body_gyro_jerk_Y', 'f_body_gyro_jerk_Z'\n",
    "        \n",
    "        axial_columns=freq_window.columns[0:12]\n",
    "        axial_df=freq_window[axial_columns]\n",
    "        freq_axial_features=[]\n",
    "        \n",
    "        for col in range(0,12,3) :         \n",
    "            # ------------------------------------------------------------\n",
    "            # 구현 가이드라인 \n",
    "            # ------------------------------------------------------------\n",
    "            # 아래 freq_3axial_features 나타난 Feature를 계산하여야 한다.\n",
    "            # 각각을 계산하기위한 함수는 'Feature_engineering.py'에 내제되어 있다.\n",
    "            # ------------------------------------------------------------\n",
    "            \n",
    "            curr_col = axial_columns[col:col+3]\n",
    "            curr_df = axial_df[curr_col]\n",
    "            \n",
    "            mean_vector = mean_axial(curr_df)\n",
    "            std_vector = std_axial(curr_df)\n",
    "            mad_vector = mad_axial(curr_df)\n",
    "            max_vector = max_axial(curr_df)\n",
    "            min_vector = min_axial(curr_df)\n",
    "            sma_value = f_sma_axial(curr_df)\n",
    "            energy_vector = f_energy_axial(curr_df)\n",
    "            IQR_vector = IQR_axial(curr_df)\n",
    "            entropy_vector = entropy_axial(curr_df)\n",
    "            max_inds_vector = f_max_Inds_axial(curr_df)\n",
    "            mean_Freq_vector = f_mean_Freq_axial(curr_df)\n",
    "            skewness_and_kurtosis_vector = f_skewness_and_kurtosis_axial(curr_df)\n",
    "            bands_energy_vector = f_all_bands_energy_axial(curr_df)\n",
    "            \n",
    "            freq_3axial_features = mean_vector +std_vector + mad_vector + max_vector + min_vector + [sma_value] + energy_vector + IQR_vector + entropy_vector + max_inds_vector + mean_Freq_vector + skewness_and_kurtosis_vector + bands_energy_vector\n",
    "            freq_axial_features = freq_axial_features+ freq_3axial_features\n",
    "        \n",
    "        ##################################################################################\n",
    "        \n",
    "        # Frequency domain - Feature extractor - Part 2. Magnitude Features\n",
    "        \n",
    "        #[12]'f_body_acc_mag'\n",
    "        #[13]'f_body_acc_jerk_mag'\n",
    "        #[14]'f_body_gyro_mag'\n",
    "        #[15]'f_body_gyro_jerk_mag'\n",
    "        \n",
    "        mag_columns=freq_window.columns[12:]\n",
    "        mag_columns=freq_window[mag_columns]\n",
    "        \n",
    "        freq_mag_features = []\n",
    "        \n",
    "        for col in mag_columns:\n",
    "            # ------------------------------------------------------------\n",
    "            # 구현 가이드라인 \n",
    "            # ------------------------------------------------------------\n",
    "            # 아래 col_mag_values에 나타난 Feature를 계산하여야 한다.\n",
    "            # 각각을 계산하기위한 함수는 'Feature_engineering.py'에 내제되어 있다.\n",
    "            # ------------------------------------------------------------\n",
    "            mean_value = mean_mag(mag_columns[col])\n",
    "            std_value = std_mag(mag_columns[col])\n",
    "            mad_value = mad_mag(mag_columns[col])\n",
    "            max_value = max_mag(mag_columns[col])\n",
    "            min_value = min_mag(mag_columns[col])\n",
    "            sma_value = t_sma_mag(mag_columns[col])\n",
    "            energy_value = t_energy_mag(mag_columns[col])\n",
    "            IQR_value = IQR_mag(mag_columns[col])\n",
    "            entropy_value = entropy_mag(mag_columns[col])\n",
    "            max_Inds_value = f_max_Inds_mag(mag_columns[col])\n",
    "            mean_Freq_value = f_mean_Freq_mag(mag_columns[col])\n",
    "            skewness_value = f_skewness_mag(mag_columns[col])\n",
    "            kurtosis_value = f_kurtosis_mag(mag_columns[col])\n",
    "            # 13 value per each t_mag_column\n",
    "            col_mag_values = [mean_value, std_value, mad_value, max_value, \n",
    "                              min_value, sma_value, energy_value,IQR_value, \n",
    "                              entropy_value, max_Inds_value, mean_Freq_value,\n",
    "                              skewness_value, kurtosis_value ]\n",
    "            \n",
    "            freq_mag_features= freq_mag_features+ col_mag_values\n",
    "        \n",
    "        ##################################################################################\n",
    "        \n",
    "        # Time domain - Feature extractor - Part 3. Additional Features \n",
    "        \n",
    "        additional_features = angle_features(time_window)\n",
    "                \n",
    "        ##################################################################################\n",
    "        \n",
    "        total_features = time_axial_features + time_mag_features + freq_axial_features + freq_mag_features + additional_features\n",
    "        \n",
    "        total_data.append(total_features)\n",
    "        if condition is 'train' :\n",
    "            total_label.append(window_activity_id)\n",
    "    \n",
    "    total_data = np.array(total_data)\n",
    "    if condition is 'train' :\n",
    "        total_label = np.array(total_label)\n",
    "    \n",
    "    if condition is 'train' :\n",
    "        return total_data, total_label\n",
    "    elif condition is 'test' :\n",
    "        return total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ede0bbcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T18:03:08.160696Z",
     "iopub.status.busy": "2022-04-22T18:03:08.160416Z",
     "iopub.status.idle": "2022-04-22T18:14:05.611796Z",
     "shell.execute_reply": "2022-04-22T18:14:05.610959Z"
    },
    "papermill": {
     "duration": 657.870036,
     "end_time": "2022-04-22T18:14:05.616198",
     "exception": false,
     "start_time": "2022-04-22T18:03:07.746162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7283/7283 [07:52<00:00, 15.42it/s]\n",
      "100%|██████████| 3116/3116 [03:04<00:00, 16.90it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label = feature_extractor(train_time_dictionary_window,train_frequent_dictionary_window,condition='train')\n",
    "test_data = feature_extractor(test_time_dictionary_window,test_frequent_dictionary_window,condition='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a6b80a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T18:14:10.246952Z",
     "iopub.status.busy": "2022-04-22T18:14:10.246695Z",
     "iopub.status.idle": "2022-04-22T18:14:10.577521Z",
     "shell.execute_reply": "2022-04-22T18:14:10.576655Z"
    },
    "papermill": {
     "duration": 2.640656,
     "end_time": "2022-04-22T18:14:10.579842",
     "exception": false,
     "start_time": "2022-04-22T18:14:07.939186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# [Empty Module #2] Data Normalization\n",
    "# -------------------------------------\n",
    "\n",
    "# -------------------------------------\n",
    "# Data Normalization\n",
    "# -------------------------------------\n",
    "# 목적: 앞서 구축한 train,test 셋에 대한 Feature를 정규화한다.\n",
    "# 입력인자: train 셋에서 추출된 Feature, test 셋에서 추출된 Feature\n",
    "# 출력인자: 정규화된 Feature Vector\n",
    "# -------------------------------------\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data)\n",
    "train_norm_data = scaler.transform(train_data)\n",
    "test_norm_data = scaler.transform(test_data)\n",
    "# ------------------------------------------------------------\n",
    "# 구현 가이드라인 \n",
    "# ------------------------------------------------------------\n",
    "# sklearn에서 제공하는 MinMaxScaler를 사용해 데이터 정규화를 진행한다.\n",
    "# (MinMaxScaler가 아닌 다른 정규화를 사용할 수 있다.)\n",
    "# ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f9cfcd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T18:14:15.187602Z",
     "iopub.status.busy": "2022-04-22T18:14:15.185851Z",
     "iopub.status.idle": "2022-04-22T18:14:29.196372Z",
     "shell.execute_reply": "2022-04-22T18:14:29.195395Z"
    },
    "papermill": {
     "duration": 16.286684,
     "end_time": "2022-04-22T18:14:29.198971",
     "exception": false,
     "start_time": "2022-04-22T18:14:12.912287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# [Empty Module #3] RandomForest를 이용한 분류\n",
    "# -------------------------------------\n",
    "\n",
    "# -------------------------------------\n",
    "# SVC를 이용한 분류\n",
    "# -------------------------------------\n",
    "# 목적: 앞서 완성한 train/test Feature를 RandomForest를 이용해 분류한다.\n",
    "# 입력인자: Feature vector(train/test)\n",
    "# 출력인자: 분류결과\n",
    "# -------------------------------------\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=200)\n",
    "clf.fit(train_norm_data, train_label)\n",
    "y_pred = clf.predict(test_norm_data)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 구현 가이드라인 \n",
    "# ------------------------------------------------------------\n",
    "# sklearn에서 제공하는 RandomForest를 사용해 데이터 정규화를 진행한다.\n",
    "# (RandomForest를가 아닌 다른 분류모델을 사용할 수 있다.)\n",
    "# ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fce152fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T18:14:33.770135Z",
     "iopub.status.busy": "2022-04-22T18:14:33.769638Z",
     "iopub.status.idle": "2022-04-22T18:14:33.790821Z",
     "shell.execute_reply": "2022-04-22T18:14:33.790269Z"
    },
    "papermill": {
     "duration": 2.323716,
     "end_time": "2022-04-22T18:14:33.792858",
     "exception": false,
     "start_time": "2022-04-22T18:14:31.469142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit_csv = pd.read_csv('/kaggle/input/2022-ml-w7p1/submit.csv')\n",
    "submit_csv['Label'] = y_pred\n",
    "submit_csv.to_csv(\"submit.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 928.027431,
   "end_time": "2022-04-22T18:14:38.053578",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-22T17:59:10.026147",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
