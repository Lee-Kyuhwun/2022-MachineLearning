{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 코드 시작 전 설명\n1. 음성 데이터에 대한 이론 및 과정을 잘 모르시는 분은 코드를 작성하기 전 overview를 한번 정독하시는 것을 권합니다.\n\n2. 스켈레톤 코드가 적혀있는 해당 ipynb 파일을 다운받아 자신의 작업공간(코랩, 캐글노트북 등등)에서 코드를 작성합니다.\n\n3. 각 코드에는 빈칸과 함께 구현 가이드 라인이 제공됩니다. 해당 가이드라인을 따라 코드를 작성하여 베이스 라인 성능을 달성합니다.\n   진행에 어려움이 있으신 경우에는 feature 가공에 사용되는 함수의 documentation 설명을 읽으시는 것을 권합니다.\n   \n4. (선택)베이스라인에 도달하였다면, 음성 feature를 새롭게 가공하여 추가적인 성능 향상을 달성해봅니다.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn\nimport os\nfrom os.path import join","metadata":{"execution":{"iopub.status.busy":"2022-06-02T04:04:00.768666Z","iopub.execute_input":"2022-06-02T04:04:00.769188Z","iopub.status.idle":"2022-06-02T04:04:01.960717Z","shell.execute_reply.started":"2022-06-02T04:04:00.769111Z","shell.execute_reply":"2022-06-02T04:04:01.959896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATA Load\n\nDATA_PATH = \"../input/2022-ml-project3\"\n# (참고) os.path.join 함수는 여러 문자열을 경로에 대한 문자열로 합쳐주는 함수입니다.\npd_train = pd.read_csv(join(DATA_PATH, 'train_data.csv'))\npd_test = pd.read_csv(join(DATA_PATH, 'test_data.csv'))\n\nprint(pd_train.info(), pd_test.info())","metadata":{"execution":{"iopub.status.busy":"2022-06-02T04:04:01.962316Z","iopub.execute_input":"2022-06-02T04:04:01.962538Z","iopub.status.idle":"2022-06-02T04:04:02.017241Z","shell.execute_reply.started":"2022-06-02T04:04:01.96251Z","shell.execute_reply":"2022-06-02T04:04:02.016414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature 추출**\n\n* extract_feature 함수는 음성 데이터를 읽어서 다음의 과정들을 거치며 feature를 추출합니다.(함수(입력) --> 출력)\n 1. Sampling&Quantization(continuos audio signal) --> Discrete audio signal\n 2. Short Time Fourier Transfrom(Discrete audio signal) --> Spectrogram\n 3. Mel-Filter(Spectrogram) --> Mel-Spectrogram\n 4. Discrete Cosine Transform(Mel-Spectrogram) --> Mel Frequency Cepstrum Coefficient\n \n위에 과정들은 모두 librosa 라이브러리에서 제공하는 함수들을 통하여 구현가능하며 사용되는 함수는 다음과 같습니다.\n\n1. librosa.load : Continuos audio signal(.wav파일)을 Discrete audio signal로 읽음. \n2. librosa.stft : Discrte audio signal을 windowing하여 프레임별로 나눈 후 FFT(Fast Fourier Transform)을 수행\n[stft_documentation](https://librosa.org/doc/latest/generated/librosa.stft.html?highlight=stft)\n3. librosa.feature.melspectrogram : stft 함수를 통하여 구한 spectrogram을 입력으로 Mel-filter를 적용함으로써 Mel-Spectrogram 생성[melspectrogram_documentation](https://librosa.org/doc/latest/generated/librosa.feature.melspectrogram.html?highlight=melspectrogram)\n4. librosa.feature.mfcc : melspectrogram 함수를 통하여 구한 Mel-spectrogram을 입력으로 DCT를 수행함으로써 MFCC를 생성 [mfcc_documentation](https://librosa.org/doc/latest/generated/librosa.feature.mfcc.html?highlight=mfcc)","metadata":{}},{"cell_type":"code","source":"import librosa\nimport glob, pickle\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport librosa, librosa.display \n\n\n# -------------------------------------\n# extract_feature(file_name): <= 코드를 추가하여 음성 feature를 추출하는 코드를 완성하세요\n# -------------------------------------\n# 목적: MFCC를 비롯한 여러 음성 feature를 추출\n# 입력인자: .wav 음성 파일의 이름\n# 출력인자: 입력 파일을 가공한 feature들 (Spectrogram, Mel-spectrogram, MFCC)\n# -------------------------------------\n\n\ndef extract_feature(file_name):\n    \n    \n    result=np.array([])\n    X, sample_rate = librosa.load(file_name, sr=22050)\n    \n    #----------step3. spectrogram을 구하세요.---------------------\n    # 구현 가이드 라인(3)\n    # ------------------------------------------------------------------------------\n    # 1. 입력 신호(X)를 librosa.stft 함수의 입력으로 하여 spectrogram을 구하세요.\n    #   -참고) 사람의 음성은 20~40(ms) 사이 시간 내에서 현재 말하는 발음을 변경할 수 없다고 합니다.\n    #       시간축에 대한 구간을 나눌 때 20~40(ms) 이내 간격으로 나누기 위하여 n_fft 파라미터 값을 조정해주세요.(베이스라인 성능은 23ms 간격으로 나누었습니다.)\n    #       정확한 조정을 위하여 librosa documentation에서 librosa.stft 함수 내 n_fft 파라미터 설명을 참조하세요.\n    #\n    # 2. spectrogram에 절대값을 취하여 복소수 형태의 값을 바꾸세요.\n    #\n    # 3. 구한 spectrogram을 학습에 사용하기 위하여 프레임 축의 평균값을 취한 뒤 spectrogram_feature에 저장해주세요\n    #   -참고) spectrogram의 shape은 (frequency의 길이, 프레임 수)로 이루어져있습니다.\n    #       (np.mean함수를 사용하여 spectrogram의 shape이 (requency의 길이)가 되면 성공.)\n    # -----------------------------------------------------------------------------\n    spectrogram = librosa.stft(X,n_fft=512)\n    ##stft함수를 이용하여 spectrogram를 구한다\n    spectrogram = np.abs(spectrogram)\n    #abs함수를 이용하여 절대값을 취한다.\n    spectrogram_feature = np.mean(spectrogram.T,axis=0)\n    #axis =0은 행 방향으로 동작한다. 그리고 mean함수를 이용하여 평균을 구한다. \n    #-------------------------------------------------------------------------------\n    \n    #----------step4. Mel-spectrogram을 구하세요.---------------------\n    # 구현 가이드 라인(4)\n    # ------------------------------------------------------------------------------\n    # 1. step3-2에서 구한 spectrogram을 제곱하여 power spectrogram을 만드세요.\n    #\n    # 2. power spectrogram을 librosa.feature.melspectrogram 함수의 입력으로 하여 mel-spectrogram을 구하세요.\n    #   - 참고) documentation을 통해 librosa.feature.melspectrogram 함수의 입력 인자를 꼭 확인하셔서 올바르게 넣어주세요.\n    #\n    # 3. step4-2에서 구한 mel-spectrogram은 power-magnitude 값입니다. librosa.power_to_db함수를 통하여 power magnitude를 데쉬벨(db)로 변환하세요.\n    #\n    # 4. 구한 mel-spectrogram을 학습에 사용하기 위하여 프레임 축의 평균값을 취한 뒤 mel_spectrogram_feature에 저장해주세요\n    #   -참고) mel-spectrogram의 shape은 (mel filter의 길이, 프레임 수)로 이루어져있습니다.\n    #       (np.mean함수를 사용하여 mel-spectrogram의 shape이 (mel filter의 길이)가 되면 성공.)\n    # -----------------------------------------------------------------------------\n    power_spectrogram = spectrogram **2\n    # 제곱을 해서 power_spectrogram를 구한다.\n    mel= librosa.feature.melspectrogram(S=power_spectrogram,sr=sample_rate)\n    # melspectrogram함수를 이용하여 mel-spectrogram를 구한다\n    mel = librosa.power_to_db(mel)\n    #power_to_db함수를 이용해서 데쉬벨로 변경한다.\n    mel_spectrogram_feature = np.mean(mel.T,axis=0)\n    # mel에 mean함수로 평균값을 취하고 mel-spectrogram-feature에 저장한다.\n    #-------------------------------------------------------------------------------\n\n    #----------step5. MFCC를 구하세요.---------------------\n    # 구현 가이드 라인(5)\n    # ------------------------------------------------------------------------------ \n    # 1. step4-3에서 데쉬벨로 변환한 mel-spectrogram을 librosa.feature.mfcc 함수의 입력으로 하여 MFCC를 구하세요.\n    #   - 참고) documentation을 통해 librosa.feature.mfcc 함수의 입력 인자를 꼭 확인하셔서 올바르게 넣어주세요.\n    #\n    # 2. 구한 MFCC 학습에 사용하기 위하여 프레임 축의 평균값을 취한 뒤 mfcc_feature에 저장해주세요\n    # -참고) MFCC shape은 (MFCC의 길이, 프레임 수)로 이루어져있습니다.\n    #       (np.mean함수를 사용하여 MFCC의 shape이 (MFCC의 길이)가 되면 성공.)\n    # -----------------------------------------------------------------------------\n    \n    mfcc = librosa.feature.mfcc(S=mel, n_mfcc=20)\n    mfcc_feature = np.mean(mfcc.T, axis=0)\n    #mfcc를 구하고 학습에 사용하기 위하여 평균을 취한다.\n    #-------------------------------------------------------------------------------\n\n    return spectrogram_feature, mel_spectrogram_feature, mfcc_feature ","metadata":{"execution":{"iopub.status.busy":"2022-06-02T04:04:02.018664Z","iopub.execute_input":"2022-06-02T04:04:02.018934Z","iopub.status.idle":"2022-06-02T04:04:03.716803Z","shell.execute_reply.started":"2022-06-02T04:04:02.018881Z","shell.execute_reply":"2022-06-02T04:04:03.716099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **데이터 불러오기**\n\n### csv파일에 저장된 파일 이름과 학습용 label을 로드하여 학습 및 평가용 데이터를 불러오세요.","metadata":{}},{"cell_type":"code","source":"#DataFlair - Load the data and extract features for each sound file\n# (참고) tqdm 은 진행률에 대한 프로그레스바를 알려주는 라이브러리입니다. \n# for문에 자주 사용되며, 얼마나 진행되었는지를 시각적으로 확인할 수 있어 자주 사용됩니다.\nfrom tqdm.notebook import tqdm\ndef load_data(data_info, isTrain=True):\n    \n    PATH = join('/kaggle','input','2022-ml-project3')\n    if isTrain:\n        train_data = {'spectrogram':[],'mel':[],'mfcc':[]}#음성 feature들을 담는 dictionary\n        train_label = []#학습에 사용할 label을 담는 list\n        \n        file_list = data_info['file_name']\n        emotion_list = data_info['emotion']\n        # (참고) zip을 통해 2개 이상의 변수를 한 번에 넘길 수 있습니다.\n        # 궁금한 점이 있으면 zip() 내장함수를 검색해보시기 바랍니다\n        for file_name, emotion in tqdm(zip(file_list, emotion_list)):\n            # ------------- step1. 학습용 데이터로더 코드를 작성하세요.----------------------\n            # 구현 가이드라인  (1)\n            # ------------------------------------------------------------\n            # train.csv 파일에 있는 음성 파일의 이름과 emotion 정보를 통하여 학습용 데이터를 로드하세요.\n            # 음성 파일의 정확한 경로 명을 extract_feature 함수의 입력으로 넣을 수 있게 경로를 잘 설정해보세요.\n            # extract_feature를 통해 구한 음성 feature들을 train_data 사전 속 배열에 알맞게 append 해주세요. ex) train_data['spectrogram'].append(spectrogram_feature)\n            # ------------------------------------------------------------\n            # 구현 가이드라인을 참고하여 코드를 작성해보세요.\n            \n            # import pdb;pdb.set_trace() 를 통한 중단점을 찍어가며 가이드라인을 완성하는 것을 권장합니다.\n            # (힌트) os.path.join 함수를 이용하여 file_name(.wav 파일) 이 있는 경로를 맞춰주어야 합니다\n            #os.path.join('..\\train_data','train_data','train_data')\n            spectrogram, mel, mfcc = extract_feature(join(\"../input/2022-ml-project3/train_data/train_data\",file_name))  \n            #tarin-data를 불러오고 그 안에 있는 각각의 성분들을 추출한다. \n            train_data['spectrogram'].append(spectrogram)\n            train_data['mel'].append(mel)\n            train_data['mfcc'].append(mfcc)\n            #data의 각 dictionary에 해당되는 값들을 추가한다. \n            train_label.append(emotion)\n            #train-label에 emotion을 추가한다.\n\n                \n            #----------------------------------------------------------------------------------------- \n            \n        return train_data, np.array(train_label)\n    \n    else:\n        test_data = {'spectrogram':[],'mel':[],'mfcc':[]}#음성 feature들을 담는 dictionary\n        file_list = data_info['file_name']\n    \n        for file_name in tqdm(file_list):\n            # -------------step2. 평가용 데이터로더 코드를 작성하세요.-----------------\n            # 구현 가이드라인  (2)\n            # ------------------------------------------------------------\n            # test.csv 파일에 있는 음성 파일의 이름정보를 통하여 평가용 데이터를 로드하세요.\n            # 음성 파일의 정확한 경로 명을 extract_feature 함수의 입력으로 넣을 수 있게 경로를 잘 설정해보세요.\n            # extract_feature를 통해 구한 음성 feature들을 test_data 사전 속 배열에 알맞게 append 해주세요. ex) test_data['spectrogram'].append(spectrogram_feature)\n            # ------------------------------------------------------------\n            # 구현 가이드라인을 참고하여 코드를 작성해보세요.\n            spectrogram, mel, mfcc = extract_feature(join(\"../input/2022-ml-project3/test_data/test_data\",file_name))\n            #test data를 extra_feature함수를 이용해서 \n            test_data['spectrogram'].append(spectrogram)\n            test_data['mel'].append(mel)\n            test_data['mfcc'].append(mfcc)\n            #각가의 딕션너리에 값들을 추가한다.\n            #----------------------------------------------------------------------------------------- \n            \n        return test_data\n\n#DataFlair - Split the dataset\ntrain_data, y_train = load_data(pd_train)\ntest_data = load_data(pd_test, isTrain=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T04:04:03.718543Z","iopub.execute_input":"2022-06-02T04:04:03.718742Z","iopub.status.idle":"2022-06-02T04:06:59.024236Z","shell.execute_reply.started":"2022-06-02T04:04:03.718714Z","shell.execute_reply":"2022-06-02T04:06:59.023617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 모델 학습 및 추론\n위에서 우리는 \n1. 음성 신호에 대하여 Short Time Fourier Transform(stft)를 통해 **spectrogram**을 구하고,\n2. 구한 spectrogram에 mel-filter를 씌워 **mel-spectrogram**을 구했으며,\n3. 마지막으로 mel-spectrogram에 Discrete cosine transform(DCT) 취해줌으로써 **mfcc**까지 계산하였습니다.\n\n위에서 구한 feature들은 모두 음성 데이터를 활용하는 다양한 작업(사람 음성 분류, 음성을 통한 감정 분류 등)에서 모델 학습에 사용할 수 있는 feature들입니다.\n각각의 **feature들에 따른 모델의 정확도가 얼마나 차이**나는지를 확인해봅시다.\n\n- 베이스라인의 분류기는 **RandomForestClassifier**를 사용합니다.\n- **random_state에 따른 성능 차이가 발생하오니 반드시 random_state를 1로 고정해주시길 바랍니다.**","metadata":{}},{"cell_type":"code","source":"#RandomForestClassifier로 음성 감정 분류 학습 및 평가\nfrom sklearn.ensemble import RandomForestClassifier\nsample = pd.read_csv(join(DATA_PATH,'sample_submit.csv'))\n\nfor feature_name in train_data.keys():\n    # ------------- step6. 음성 feature들을 가지고 모델을 학습하세요.----------------------\n    # 구현 가이드라인  (6)\n    # ------------------------------------------------------------\n    # dictionary 형태의 train_data 변수 내에는 spectrogram, mel-spectrogram, mfcc feature들이 존재합니다.\n    # 반복문을 통해 각 종류의 feature를 하나씩 불러오세요. ex) x_train = np.array(train_data[feature_name])\n    # 불러온 feature로 모델을 학습 및 추론 후 sample submit파일에 저장하세요.\n    # ------------------------------------------------------------\n    # 구현 가이드라인을 참고하여 코드를 작성해보세요.\n    x_train = np.array(train_data[feature_name])\n    x_test = np.array(test_data[feature_name])\n    #x_train과 x_test에 각각의 feature을 넣는다.\n    rfc = RandomForestClassifier(random_state = 1)\n    rfc.fit(x_train,y_train)\n    predict = rfc.predict(x_test)\n    # RandomForestClassifier를 이용하여 학습한다.\n    \n    \n    \n    #Sample submit file 저장\n    sample['emotion'] = predict.reshape(-1,1)\n    sample.to_csv(join(feature_name+'.csv'),index=False,header=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T04:06:59.025567Z","iopub.execute_input":"2022-06-02T04:06:59.025955Z","iopub.status.idle":"2022-06-02T04:07:02.292952Z","shell.execute_reply.started":"2022-06-02T04:06:59.025901Z","shell.execute_reply":"2022-06-02T04:07:02.292124Z"},"trusted":true},"execution_count":null,"outputs":[]}]}